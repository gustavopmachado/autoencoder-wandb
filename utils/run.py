"""Script for training"""

# pylint: disable=import-error, no-name-in-module, redefined-outer-name, consider-using-f-string

import os
import pickle

import wandb
import yaml
from torch.distributed import destroy_process_group

from models.autoencoder import instantiate
from utils.settings import ddp
from utils.training import AETrainer


def run(hyperparameters):
    """Perform the training of a model and logged via wandb.
    A dictionary is used as a single argument for wandb, however its
    keys and respective descriptions are listed below as paramters.

    Parameters
    ----------
    activation : str
        Activation function. Available is "LeakyReLU", "ReLU",
        "Tanh", "Mish" and "Sigmoid"

    attention : List[int]
        List of features in which attention will be applied

    autocast : bool
        Whether to enable autocast

    architecture : str
        Architecture to be used in the model. Values are UNet, MGUnet

    batch : int
        Batch size

    betas : tuple
        Adam's betas

    channels : int
        Input channel size

    criterion : str
        Loss function

    dataset : str
        Name of the datafolder in data/

    depth : int, optional
        Depth of the semi-reconstruction layer in relation to the first level or surface

    embeddings : int, optional
        Number of features in the posterior's embedded dimension for the VAE

    epochs : int
        Training epochs

    eta : float, optional
        Kullbackâ€“Leibler divergence regularization factor

    features : List[int]
            List of features to be generated throughout the compressive layers

    gamma : float
        Percentage of features that will be generated by
        anti-aliased convolution operations

    groupnorm : int
        Number of groups for GroupNorm. If -1, applies BatchNorm2d

    latent : int, optional
        Number of features in the latent space for the VAE

    lr : float
        Learning rate

    ratio : float
        Percentage of data to be used for validation

    resnet : int
        Number of ResNet blocks per layer

    savefreq : int
        Model save frequency

    scheduler : object
        Schedulet object

    Returns
    -------
        Trained model

    """
    # Initialise DDP
    ddp()

    # Initialise wandb for logging
    with wandb.init(project="msc-thesis",
                    config=hyperparameters,
                    name=("GPU: " + str(int(os.environ["LOCAL_RANK"]))),
                    ):

        # Ensure logging matches execution via wandb
        config = wandb.config

        # Set up training
        if config.get("architecture") == "AE":
            model = instantiate(**config)
            trainer = AETrainer(model, **config)
        else:
            raise ValueError("Training not implemented for {}".format(config.get("architecture")))

        # Save hyperparameters
        if int(os.environ["LOCAL_RANK"]) == int(os.environ["MAIN_RANK"]):
            path = os.path.join(trainer.savepath, "hyperparameters.pkl")
            with open(path, 'wb') as f:
                pickle.dump(hyperparameters, f)

        # Traning
        model = trainer.run()

    # DDP tear down
    destroy_process_group()

    return model


if __name__ == "__main__":
    # Parameters
    with open("./experiments/config.yaml", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    hyperparameters = {}
    for v in config.values():
        hyperparameters.update(v)

    # Training
    model = run(hyperparameters)
