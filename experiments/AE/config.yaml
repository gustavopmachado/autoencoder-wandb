model:
  activation: ReLU
  features: [512, 256, 128]
  latent: 2
optimiser:
  betas: [.9, .999]
training:
  batch: 128
  epochs: 100
  lr: 5.0e-4
wandb:
  logfreq: 1
  plotfreq: 10

